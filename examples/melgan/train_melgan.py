{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPjkY+LYAMGEG4PzAYUsFOx"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ls7IWxJsAtYP","executionInfo":{"status":"ok","timestamp":1666430989119,"user_tz":-330,"elapsed":185832,"user":{"displayName":"Bellamkonda Ieeshasree","userId":"13883732212391436593"}},"outputId":"32a3e618-07c4-4667-ba29-a8cdbf862538"},"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'TTS-FASTSPEECH'...\n","remote: Enumerating objects: 351, done.\u001b[K\n","remote: Counting objects: 100% (228/228), done.\u001b[K\n","remote: Compressing objects: 100% (164/164), done.\u001b[K\n","remote: Total 351 (delta 63), reused 199 (delta 52), pack-reused 123\u001b[K\n","Receiving objects: 100% (351/351), 3.53 MiB | 12.59 MiB/s, done.\n","Resolving deltas: 100% (96/96), done.\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Processing /content/TTS-FASTSPEECH\n","\u001b[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n","   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\n","Collecting tensorflow-gpu==2.7.0\n","  Downloading tensorflow_gpu-2.7.0-cp37-cp37m-manylinux2010_x86_64.whl (489.6 MB)\n","\u001b[K     |████████████████████████████████| 489.6 MB 24 kB/s \n","\u001b[?25hCollecting tensorflow-addons>=0.10.0\n","  Downloading tensorflow_addons-0.18.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n","\u001b[K     |████████████████████████████████| 1.1 MB 56.2 MB/s \n","\u001b[?25hRequirement already satisfied: setuptools>=38.5.1 in /usr/local/lib/python3.7/dist-packages (from TensorFlowTTS==0.0) (57.4.0)\n","Collecting huggingface_hub==0.0.8\n","  Downloading huggingface_hub-0.0.8-py3-none-any.whl (34 kB)\n","Requirement already satisfied: librosa>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from TensorFlowTTS==0.0) (0.8.1)\n","Requirement already satisfied: soundfile>=0.10.2 in /usr/local/lib/python3.7/dist-packages (from TensorFlowTTS==0.0) (0.11.0)\n","Requirement already satisfied: matplotlib>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from TensorFlowTTS==0.0) (3.2.2)\n","Requirement already satisfied: PyYAML>=3.12 in /usr/local/lib/python3.7/dist-packages (from TensorFlowTTS==0.0) (6.0)\n","Requirement already satisfied: tqdm>=4.26.1 in /usr/local/lib/python3.7/dist-packages (from TensorFlowTTS==0.0) (4.64.1)\n","Requirement already satisfied: h5py>=2.10.0 in /usr/local/lib/python3.7/dist-packages (from TensorFlowTTS==0.0) (3.1.0)\n","Collecting unidecode>=1.1.1\n","  Downloading Unidecode-1.3.6-py3-none-any.whl (235 kB)\n","\u001b[K     |████████████████████████████████| 235 kB 58.8 MB/s \n","\u001b[?25hCollecting inflect>=4.1.0\n","  Downloading inflect-6.0.2-py3-none-any.whl (34 kB)\n","Requirement already satisfied: scikit-learn>=0.22.0 in /usr/local/lib/python3.7/dist-packages (from TensorFlowTTS==0.0) (1.0.2)\n","Collecting pyworld>=0.2.10\n","  Downloading pyworld-0.3.1.tar.gz (72 kB)\n","\u001b[K     |████████████████████████████████| 72 kB 1.4 MB/s \n","\u001b[?25hRequirement already satisfied: numba>=0.48 in /usr/local/lib/python3.7/dist-packages (from TensorFlowTTS==0.0) (0.56.3)\n","Collecting jamo>=0.4.1\n","  Downloading jamo-0.4.1-py3-none-any.whl (9.5 kB)\n","Collecting pypinyin\n","  Downloading pypinyin-0.47.1-py2.py3-none-any.whl (1.4 MB)\n","\u001b[K     |████████████████████████████████| 1.4 MB 51.9 MB/s \n","\u001b[?25hCollecting g2pM\n","  Downloading g2pM-0.1.2.5-py3-none-any.whl (1.7 MB)\n","\u001b[K     |████████████████████████████████| 1.7 MB 39.9 MB/s \n","\u001b[?25hCollecting textgrid\n","  Downloading TextGrid-1.5-py3-none-any.whl (10.0 kB)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from TensorFlowTTS==0.0) (7.1.2)\n","Collecting g2p_en\n","  Downloading g2p_en-2.1.0-py3-none-any.whl (3.1 MB)\n","\u001b[K     |████████████████████████████████| 3.1 MB 40.4 MB/s \n","\u001b[?25hCollecting dataclasses\n","  Downloading dataclasses-0.6-py3-none-any.whl (14 kB)\n","Collecting pyopenjtalk\n","  Downloading pyopenjtalk-0.3.0.tar.gz (1.5 MB)\n","\u001b[K     |████████████████████████████████| 1.5 MB 56.5 MB/s \n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface_hub==0.0.8->TensorFlowTTS==0.0) (3.8.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from huggingface_hub==0.0.8->TensorFlowTTS==0.0) (2.23.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from huggingface_hub==0.0.8->TensorFlowTTS==0.0) (4.13.0)\n","Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.7.0->TensorFlowTTS==0.0) (1.21.6)\n","Requirement already satisfied: tensorboard~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.7.0->TensorFlowTTS==0.0) (2.9.1)\n","Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.7.0->TensorFlowTTS==0.0) (1.14.1)\n","Collecting tensorflow-estimator<2.8,~=2.7.0rc0\n","  Downloading tensorflow_estimator-2.7.0-py2.py3-none-any.whl (463 kB)\n","\u001b[K     |████████████████████████████████| 463 kB 47.9 MB/s \n","\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.7.0->TensorFlowTTS==0.0) (1.1.2)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.7.0->TensorFlowTTS==0.0) (2.0.1)\n","Requirement already satisfied: gast<0.5.0,>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.7.0->TensorFlowTTS==0.0) (0.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.7.0->TensorFlowTTS==0.0) (1.6.3)\n","Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.7.0->TensorFlowTTS==0.0) (1.3.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.7.0->TensorFlowTTS==0.0) (4.1.1)\n","Requirement already satisfied: wheel<1.0,>=0.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.7.0->TensorFlowTTS==0.0) (0.37.1)\n","Requirement already satisfied: flatbuffers<3.0,>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.7.0->TensorFlowTTS==0.0) (1.12)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.7.0->TensorFlowTTS==0.0) (0.27.0)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.7.0->TensorFlowTTS==0.0) (1.49.1)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.7.0->TensorFlowTTS==0.0) (1.15.0)\n","Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.7.0->TensorFlowTTS==0.0) (14.0.6)\n","Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.7.0->TensorFlowTTS==0.0) (3.17.3)\n","Collecting keras<2.8,>=2.7.0rc0\n","  Downloading keras-2.7.0-py2.py3-none-any.whl (1.3 MB)\n","\u001b[K     |████████████████████████████████| 1.3 MB 68.1 MB/s \n","\u001b[?25hRequirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.7.0->TensorFlowTTS==0.0) (0.2.0)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.7.0->TensorFlowTTS==0.0) (3.3.0)\n","Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.10.0->TensorFlowTTS==0.0) (1.5.2)\n","Requirement already satisfied: pydantic>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from inflect>=4.1.0->TensorFlowTTS==0.0) (1.9.2)\n","Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.7/dist-packages (from librosa>=0.7.0->TensorFlowTTS==0.0) (1.2.0)\n","Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.7/dist-packages (from librosa>=0.7.0->TensorFlowTTS==0.0) (1.6.0)\n","Requirement already satisfied: decorator>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa>=0.7.0->TensorFlowTTS==0.0) (4.4.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from librosa>=0.7.0->TensorFlowTTS==0.0) (21.3)\n","Requirement already satisfied: audioread>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa>=0.7.0->TensorFlowTTS==0.0) (3.0.0)\n","Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa>=0.7.0->TensorFlowTTS==0.0) (1.7.3)\n","Requirement already satisfied: resampy>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from librosa>=0.7.0->TensorFlowTTS==0.0) (0.4.2)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.0->TensorFlowTTS==0.0) (2.8.2)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.0->TensorFlowTTS==0.0) (1.4.4)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.0->TensorFlowTTS==0.0) (3.0.9)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.0->TensorFlowTTS==0.0) (0.11.0)\n","Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.48->TensorFlowTTS==0.0) (0.39.1)\n","Requirement already satisfied: appdirs>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from pooch>=1.0->librosa>=0.7.0->TensorFlowTTS==0.0) (1.4.4)\n","Requirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (from pyworld>=0.2.10->TensorFlowTTS==0.0) (0.29.32)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface_hub==0.0.8->TensorFlowTTS==0.0) (2022.9.24)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface_hub==0.0.8->TensorFlowTTS==0.0) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface_hub==0.0.8->TensorFlowTTS==0.0) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface_hub==0.0.8->TensorFlowTTS==0.0) (3.0.4)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22.0->TensorFlowTTS==0.0) (3.1.0)\n","Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.7/dist-packages (from soundfile>=0.10.2->TensorFlowTTS==0.0) (1.15.1)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.0->soundfile>=0.10.2->TensorFlowTTS==0.0) (2.21)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow-gpu==2.7.0->TensorFlowTTS==0.0) (0.6.1)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow-gpu==2.7.0->TensorFlowTTS==0.0) (1.0.1)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow-gpu==2.7.0->TensorFlowTTS==0.0) (3.4.1)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow-gpu==2.7.0->TensorFlowTTS==0.0) (1.8.1)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow-gpu==2.7.0->TensorFlowTTS==0.0) (1.35.0)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow-gpu==2.7.0->TensorFlowTTS==0.0) (0.4.6)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow-gpu==2.7.0->TensorFlowTTS==0.0) (0.2.8)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow-gpu==2.7.0->TensorFlowTTS==0.0) (4.9)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow-gpu==2.7.0->TensorFlowTTS==0.0) (4.2.4)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow-gpu==2.7.0->TensorFlowTTS==0.0) (1.3.1)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->huggingface_hub==0.0.8->TensorFlowTTS==0.0) (3.9.0)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow-gpu==2.7.0->TensorFlowTTS==0.0) (0.4.8)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow-gpu==2.7.0->TensorFlowTTS==0.0) (3.2.1)\n","Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons>=0.10.0->TensorFlowTTS==0.0) (2.7.1)\n","Collecting distance>=0.1.3\n","  Downloading Distance-0.1.3.tar.gz (180 kB)\n","\u001b[K     |████████████████████████████████| 180 kB 54.5 MB/s \n","\u001b[?25hRequirement already satisfied: nltk>=3.2.4 in /usr/local/lib/python3.7/dist-packages (from g2p_en->TensorFlowTTS==0.0) (3.7)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk>=3.2.4->g2p_en->TensorFlowTTS==0.0) (2022.6.2)\n","Building wheels for collected packages: TensorFlowTTS, pyworld, distance, pyopenjtalk\n","  Building wheel for TensorFlowTTS (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for TensorFlowTTS: filename=TensorFlowTTS-0.0-py3-none-any.whl size=133233 sha256=86af233cda328072813fdd0c0d043be2fa692bc003fe68aed3fb95159fbefbc5\n","  Stored in directory: /root/.cache/pip/wheels/f7/81/ad/d484374a8644b9dce1eca492b108979caa2199c0b7185e03d6\n","  Building wheel for pyworld (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyworld: filename=pyworld-0.3.1-cp37-cp37m-linux_x86_64.whl size=612153 sha256=81ef4b24530182ed06a004af9057e69dbf69941c10250689fbd360486a331597\n","  Stored in directory: /root/.cache/pip/wheels/3c/23/da/9c5f21eaa1dedc0434d00411ec949b3cb678f8ac3b07a1764b\n","  Building wheel for distance (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for distance: filename=Distance-0.1.3-py3-none-any.whl size=16276 sha256=8a431e18e478a0fc5200ba8db429e07a1ca8ccac54e63799d6b6ba8075320db1\n","  Stored in directory: /root/.cache/pip/wheels/b2/10/1b/96fca621a1be378e2fe104cfb0d160bb6cdf3d04a3d35266cc\n","  Building wheel for pyopenjtalk (PEP 517) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyopenjtalk: filename=pyopenjtalk-0.3.0-cp37-cp37m-linux_x86_64.whl size=4448551 sha256=176c27c91162d84d103580a2601f108b032ced3ae736b1f91cd2d18848441d0a\n","  Stored in directory: /root/.cache/pip/wheels/b9/bd/00/a1dcdfe0afa9843945f231c3a1f6ae2a21a301a29c63e8252d\n","Successfully built TensorFlowTTS pyworld distance pyopenjtalk\n","Installing collected packages: tensorflow-estimator, keras, inflect, distance, unidecode, textgrid, tensorflow-gpu, tensorflow-addons, pyworld, pypinyin, pyopenjtalk, jamo, huggingface-hub, g2pM, g2p-en, dataclasses, TensorFlowTTS\n","  Attempting uninstall: tensorflow-estimator\n","    Found existing installation: tensorflow-estimator 2.9.0\n","    Uninstalling tensorflow-estimator-2.9.0:\n","      Successfully uninstalled tensorflow-estimator-2.9.0\n","  Attempting uninstall: keras\n","    Found existing installation: keras 2.9.0\n","    Uninstalling keras-2.9.0:\n","      Successfully uninstalled keras-2.9.0\n","  Attempting uninstall: inflect\n","    Found existing installation: inflect 2.1.0\n","    Uninstalling inflect-2.1.0:\n","      Successfully uninstalled inflect-2.1.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","tensorflow 2.9.2 requires keras<2.10.0,>=2.9.0rc0, but you have keras 2.7.0 which is incompatible.\n","tensorflow 2.9.2 requires tensorflow-estimator<2.10.0,>=2.9.0rc0, but you have tensorflow-estimator 2.7.0 which is incompatible.\u001b[0m\n","Successfully installed TensorFlowTTS-0.0 dataclasses-0.6 distance-0.1.3 g2p-en-2.1.0 g2pM-0.1.2.5 huggingface-hub-0.0.8 inflect-6.0.2 jamo-0.4.1 keras-2.7.0 pyopenjtalk-0.3.0 pypinyin-0.47.1 pyworld-0.3.1 tensorflow-addons-0.18.0 tensorflow-estimator-2.7.0 tensorflow-gpu-2.7.0 textgrid-1.5 unidecode-1.3.6\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/tensorflow_addons/utils/ensure_tf_install.py:67: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.8.0 and strictly below 2.11.0 (nightly versions are not supported). \n"," The versions of TensorFlow you are currently using is 2.7.0 and is not supported. \n","Some things might work, some things might not.\n","If you were to encounter a bug, do not file an issue.\n","If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n","You can find the compatibility matrix in TensorFlow Addon's readme:\n","https://github.com/tensorflow/addons\n","  UserWarning,\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n","[nltk_data] Downloading package cmudict to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/cmudict.zip.\n"]}],"source":["import os\n","!git clone https://github.com/ieesha24/TTS-FASTSPEECH.git\n","os.chdir(\"TTS-FASTSPEECH\")\n","!pip install  .\n","os.chdir(\"..\")\n","import sys\n","sys.path.append(\"TTS-FASTSPEECH/\")\n","\n","import tensorflow as tf\n","\n","physical_devices = tf.config.list_physical_devices(\"GPU\")\n","for i in range(len(physical_devices)):\n","    tf.config.experimental.set_memory_growth(physical_devices[i], True)\n","\n","import sys\n","\n","sys.path.append(\".\")\n","\n","import argparse\n","import logging\n","import os\n","\n","import numpy as np\n","import soundfile as sf\n","import yaml\n","from tqdm import tqdm\n","\n","import tensorflow_tts\n","import tensorflow_tts.configs.melgan as MELGAN_CONFIG\n","from examples.melgan.audio_mel_dataset import AudioMelDataset\n","from tensorflow_tts.losses import TFMelSpectrogram\n","from tensorflow_tts.models import TFMelGANGenerator, TFMelGANMultiScaleDiscriminator\n","from tensorflow_tts.trainers import GanBasedTrainer\n","from tensorflow_tts.utils import calculate_2d_loss, calculate_3d_loss, return_strategy\n","\n","\n","class MelganTrainer(GanBasedTrainer):\n","    \"\"\"Melgan Trainer class based on GanBasedTrainer.\"\"\"\n","\n","    def __init__(\n","        self,\n","        config,\n","        strategy,\n","        steps=0,\n","        epochs=0,\n","        is_generator_mixed_precision=False,\n","        is_discriminator_mixed_precision=False,\n","    ):\n","        \"\"\"Initialize trainer.\n","        Args:\n","            steps (int): Initial global steps.\n","            epochs (int): Initial global epochs.\n","            config (dict): Config dict loaded from yaml format configuration file.\n","            is_generator_mixed_precision (bool): Use mixed precision for generator or not.\n","            is_discriminator_mixed_precision (bool): Use mixed precision for discriminator or not.\n","        \"\"\"\n","        super(MelganTrainer, self).__init__(\n","            steps,\n","            epochs,\n","            config,\n","            strategy,\n","            is_generator_mixed_precision,\n","            is_discriminator_mixed_precision,\n","        )\n","        # define metrics to aggregates data and use tf.summary logs them\n","        self.list_metrics_name = [\n","            \"adversarial_loss\",\n","            \"fm_loss\",\n","            \"gen_loss\",\n","            \"real_loss\",\n","            \"fake_loss\",\n","            \"dis_loss\",\n","            \"mels_spectrogram_loss\",\n","        ]\n","        self.init_train_eval_metrics(self.list_metrics_name)\n","        self.reset_states_train()\n","        self.reset_states_eval()\n","\n","        self.config = config\n","\n","    def compile(self, gen_model, dis_model, gen_optimizer, dis_optimizer):\n","        super().compile(gen_model, dis_model, gen_optimizer, dis_optimizer)\n","        # define loss\n","        self.mse_loss = tf.keras.losses.MeanSquaredError(\n","            reduction=tf.keras.losses.Reduction.NONE\n","        )\n","        self.mae_loss = tf.keras.losses.MeanAbsoluteError(\n","            reduction=tf.keras.losses.Reduction.NONE\n","        )\n","        self.mels_loss = TFMelSpectrogram()\n","\n","    def compute_per_example_generator_losses(self, batch, outputs):\n","        \"\"\"Compute per example generator losses and return dict_metrics_losses\n","        Note that all element of the loss MUST has a shape [batch_size] and \n","        the keys of dict_metrics_losses MUST be in self.list_metrics_name.\n","        Args:\n","            batch: dictionary batch input return from dataloader\n","            outputs: outputs of the model\n","        \n","        Returns:\n","            per_example_losses: per example losses for each GPU, shape [B]\n","            dict_metrics_losses: dictionary loss.\n","        \"\"\"\n","        audios = batch[\"audios\"]\n","        y_hat = outputs\n","\n","        p_hat = self._discriminator(y_hat)\n","        p = self._discriminator(tf.expand_dims(audios, 2))\n","        adv_loss = 0.0\n","        for i in range(len(p_hat)):\n","            adv_loss += calculate_3d_loss(\n","                tf.ones_like(p_hat[i][-1]), p_hat[i][-1], loss_fn=self.mse_loss\n","            )\n","        adv_loss /= i + 1\n","\n","        # define feature-matching loss\n","        fm_loss = 0.0\n","        for i in range(len(p_hat)):\n","            for j in range(len(p_hat[i]) - 1):\n","                fm_loss += calculate_3d_loss(\n","                    p[i][j], p_hat[i][j], loss_fn=self.mae_loss\n","                )\n","        fm_loss /= (i + 1) * (j + 1)\n","        adv_loss += self.config[\"lambda_feat_match\"] * fm_loss\n","\n","        per_example_losses = adv_loss\n","\n","        dict_metrics_losses = {\n","            \"adversarial_loss\": adv_loss,\n","            \"fm_loss\": fm_loss,\n","            \"gen_loss\": adv_loss,\n","            \"mels_spectrogram_loss\": calculate_2d_loss(\n","                audios, tf.squeeze(y_hat, -1), loss_fn=self.mels_loss\n","            ),\n","        }\n","\n","        return per_example_losses, dict_metrics_losses\n","\n","    def compute_per_example_discriminator_losses(self, batch, gen_outputs):\n","        audios = batch[\"audios\"]\n","        y_hat = gen_outputs\n","\n","        y = tf.expand_dims(audios, 2)\n","        p = self._discriminator(y)\n","        p_hat = self._discriminator(y_hat)\n","\n","        real_loss = 0.0\n","        fake_loss = 0.0\n","        for i in range(len(p)):\n","            real_loss += calculate_3d_loss(\n","                tf.ones_like(p[i][-1]), p[i][-1], loss_fn=self.mse_loss\n","            )\n","            fake_loss += calculate_3d_loss(\n","                tf.zeros_like(p_hat[i][-1]), p_hat[i][-1], loss_fn=self.mse_loss\n","            )\n","        real_loss /= i + 1\n","        fake_loss /= i + 1\n","        dis_loss = real_loss + fake_loss\n","\n","        # calculate per_example_losses and dict_metrics_losses\n","        per_example_losses = dis_loss\n","\n","        dict_metrics_losses = {\n","            \"real_loss\": real_loss,\n","            \"fake_loss\": fake_loss,\n","            \"dis_loss\": dis_loss,\n","        }\n","\n","        return per_example_losses, dict_metrics_losses\n","\n","    def generate_and_save_intermediate_result(self, batch):\n","        \"\"\"Generate and save intermediate result.\"\"\"\n","        import matplotlib.pyplot as plt\n","\n","        # generate\n","        y_batch_ = self.one_step_predict(batch)\n","        y_batch = batch[\"audios\"]\n","        utt_ids = batch[\"utt_ids\"]\n","\n","        # convert to tensor.\n","        # here we just take a sample at first replica.\n","        try:\n","            y_batch_ = y_batch_.values[0].numpy()\n","            y_batch = y_batch.values[0].numpy()\n","            utt_ids = utt_ids.values[0].numpy()\n","        except Exception:\n","            y_batch_ = y_batch_.numpy()\n","            y_batch = y_batch.numpy()\n","            utt_ids = utt_ids.numpy()\n","\n","        # check directory\n","        dirname = os.path.join(self.config[\"outdir\"], f\"predictions/{self.steps}steps\")\n","        if not os.path.exists(dirname):\n","            os.makedirs(dirname)\n","\n","        for idx, (y, y_) in enumerate(zip(y_batch, y_batch_), 0):\n","            # convert to ndarray\n","            y, y_ = tf.reshape(y, [-1]).numpy(), tf.reshape(y_, [-1]).numpy()\n","\n","            # plit figure and save it\n","            utt_id = utt_ids[idx]\n","            figname = os.path.join(dirname, f\"{utt_id}.png\")\n","            plt.subplot(2, 1, 1)\n","            plt.plot(y)\n","            plt.title(\"groundtruth speech\")\n","            plt.subplot(2, 1, 2)\n","            plt.plot(y_)\n","            plt.title(f\"generated speech @ {self.steps} steps\")\n","            plt.tight_layout()\n","            plt.savefig(figname)\n","            plt.close()\n","\n","            # save as wavefile\n","            y = np.clip(y, -1, 1)\n","            y_ = np.clip(y_, -1, 1)\n","            sf.write(\n","                figname.replace(\".png\", \"_ref.wav\"),\n","                y,\n","                self.config[\"sampling_rate\"],\n","                \"PCM_16\",\n","            )\n","            sf.write(\n","                figname.replace(\".png\", \"_gen.wav\"),\n","                y_,\n","                self.config[\"sampling_rate\"],\n","                \"PCM_16\",\n","            )\n","\n","\n","def collater(\n","    items,\n","    batch_max_steps=tf.constant(8192, dtype=tf.int32),\n","    hop_size=tf.constant(256, dtype=tf.int32),\n","):\n","    \"\"\"Initialize collater (mapping function) for Tensorflow Audio-Mel Dataset.\n","    Args:\n","        batch_max_steps (int): The maximum length of input signal in batch.\n","        hop_size (int): Hop size of auxiliary features.\n","    \"\"\"\n","    audio, mel = items[\"audios\"], items[\"mels\"]\n","\n","    if batch_max_steps is None:\n","        batch_max_steps = (tf.shape(audio)[0] // hop_size) * hop_size\n","\n","    batch_max_frames = batch_max_steps // hop_size\n","    if len(audio) < len(mel) * hop_size:\n","        audio = tf.pad(audio, [[0, len(mel) * hop_size - len(audio)]])\n","\n","    if len(mel) > batch_max_frames:\n","        # randomly pickup with the batch_max_steps length of the part\n","        interval_start = 0\n","        interval_end = len(mel) - batch_max_frames\n","        start_frame = tf.random.uniform(\n","            shape=[], minval=interval_start, maxval=interval_end, dtype=tf.int32\n","        )\n","        start_step = start_frame * hop_size\n","        audio = audio[start_step : start_step + batch_max_steps]\n","        mel = mel[start_frame : start_frame + batch_max_frames, :]\n","    else:\n","        audio = tf.pad(audio, [[0, batch_max_steps - len(audio)]])\n","        mel = tf.pad(mel, [[0, batch_max_frames - len(mel)], [0, 0]])\n","\n","    items = {\n","        \"utt_ids\": items[\"utt_ids\"],\n","        \"audios\": audio,\n","        \"mels\": mel,\n","        \"mel_lengths\": len(mel),\n","        \"audio_lengths\": len(audio),\n","    }\n","\n","    return items\n","\n","\n","def main():\n","    \"\"\"Run training process.\"\"\"\n","    parser = argparse.ArgumentParser(\n","        description=\"Train MelGAN (See detail in tensorflow_tts/bin/train-melgan.py)\"\n","    )\n","    parser.add_argument(\n","        \"--train-dir\",\n","        default=None,\n","        type=str,\n","        help=\"directory including training data. \",\n","    )\n","    parser.add_argument(\n","        \"--dev-dir\",\n","        default=None,\n","        type=str,\n","        help=\"directory including development data. \",\n","    )\n","    parser.add_argument(\n","        \"--use-norm\", default=1, type=int, help=\"use norm mels for training or raw.\"\n","    )\n","    parser.add_argument(\n","        \"--outdir\", type=str, required=True, help=\"directory to save checkpoints.\"\n","    )\n","    parser.add_argument(\n","        \"--config\", type=str, required=True, help=\"yaml format configuration file.\"\n","    )\n","    parser.add_argument(\n","        \"--resume\",\n","        default=\"\",\n","        type=str,\n","        nargs=\"?\",\n","        help='checkpoint file path to resume training. (default=\"\")',\n","    )\n","    parser.add_argument(\n","        \"--verbose\",\n","        type=int,\n","        default=1,\n","        help=\"logging level. higher is more logging. (default=1)\",\n","    )\n","    parser.add_argument(\n","        \"--generator_mixed_precision\",\n","        default=0,\n","        type=int,\n","        help=\"using mixed precision for generator or not.\",\n","    )\n","    parser.add_argument(\n","        \"--discriminator_mixed_precision\",\n","        default=0,\n","        type=int,\n","        help=\"using mixed precision for discriminator or not.\",\n","    )\n","    parser.add_argument(\n","        \"--pretrained\",\n","        default=\"\",\n","        type=str,\n","        nargs=\"?\",\n","        help=\"path of .h5 melgan generator to load weights from\",\n","    )\n","    args = parser.parse_args()\n","\n","    # return strategy\n","    STRATEGY = return_strategy()\n","\n","    # set mixed precision config\n","    if args.generator_mixed_precision == 1 or args.discriminator_mixed_precision == 1:\n","        tf.config.optimizer.set_experimental_options({\"auto_mixed_precision\": True})\n","\n","    args.generator_mixed_precision = bool(args.generator_mixed_precision)\n","    args.discriminator_mixed_precision = bool(args.discriminator_mixed_precision)\n","\n","    args.use_norm = bool(args.use_norm)\n","\n","    # set logger\n","    if args.verbose > 1:\n","        logging.basicConfig(\n","            level=logging.DEBUG,\n","            stream=sys.stdout,\n","            format=\"%(asctime)s (%(module)s:%(lineno)d) %(levelname)s: %(message)s\",\n","        )\n","    elif args.verbose > 0:\n","        logging.basicConfig(\n","            level=logging.INFO,\n","            stream=sys.stdout,\n","            format=\"%(asctime)s (%(module)s:%(lineno)d) %(levelname)s: %(message)s\",\n","        )\n","    else:\n","        logging.basicConfig(\n","            level=logging.WARN,\n","            stream=sys.stdout,\n","            format=\"%(asctime)s (%(module)s:%(lineno)d) %(levelname)s: %(message)s\",\n","        )\n","        logging.warning(\"Skip DEBUG/INFO messages\")\n","\n","    # check directory existence\n","    if not os.path.exists(args.outdir):\n","        os.makedirs(args.outdir)\n","\n","    # check arguments\n","    if args.train_dir is None:\n","        raise ValueError(\"Please specify --train-dir\")\n","    if args.dev_dir is None:\n","        raise ValueError(\"Please specify either --valid-dir\")\n","\n","    # load and save config\n","    with open(args.config) as f:\n","        config = yaml.load(f, Loader=yaml.Loader)\n","    config.update(vars(args))\n","    config[\"version\"] = tensorflow_tts.__version__\n","    with open(os.path.join(args.outdir, \"config.yml\"), \"w\") as f:\n","        yaml.dump(config, f, Dumper=yaml.Dumper)\n","    for key, value in config.items():\n","        logging.info(f\"{key} = {value}\")\n","\n","    # get dataset\n","    if config[\"remove_short_samples\"]:\n","        mel_length_threshold = config[\"batch_max_steps\"] // config[\n","            \"hop_size\"\n","        ] + 2 * config[\"melgan_generator_params\"].get(\"aux_context_window\", 0)\n","    else:\n","        mel_length_threshold = None\n","\n","    if config[\"format\"] == \"npy\":\n","        audio_query = \"*-wave.npy\"\n","        mel_query = \"*-raw-feats.npy\" if args.use_norm is False else \"*-norm-feats.npy\"\n","        audio_load_fn = np.load\n","        mel_load_fn = np.load\n","    else:\n","        raise ValueError(\"Only npy are supported.\")\n","\n","    # define train/valid dataset\n","    train_dataset = AudioMelDataset(\n","        root_dir=args.train_dir,\n","        audio_query=audio_query,\n","        mel_query=mel_query,\n","        audio_load_fn=audio_load_fn,\n","        mel_load_fn=mel_load_fn,\n","        mel_length_threshold=mel_length_threshold,\n","    ).create(\n","        is_shuffle=config[\"is_shuffle\"],\n","        map_fn=lambda items: collater(\n","            items,\n","            batch_max_steps=tf.constant(config[\"batch_max_steps\"], dtype=tf.int32),\n","            hop_size=tf.constant(config[\"hop_size\"], dtype=tf.int32),\n","        ),\n","        allow_cache=config[\"allow_cache\"],\n","        batch_size=config[\"batch_size\"]\n","        * STRATEGY.num_replicas_in_sync\n","        * config[\"gradient_accumulation_steps\"],\n","    )\n","\n","    valid_dataset = AudioMelDataset(\n","        root_dir=args.dev_dir,\n","        audio_query=audio_query,\n","        mel_query=mel_query,\n","        audio_load_fn=audio_load_fn,\n","        mel_load_fn=mel_load_fn,\n","        mel_length_threshold=mel_length_threshold,\n","    ).create(\n","        is_shuffle=config[\"is_shuffle\"],\n","        map_fn=lambda items: collater(\n","            items,\n","            batch_max_steps=tf.constant(\n","                config[\"batch_max_steps_valid\"], dtype=tf.int32\n","            ),\n","            hop_size=tf.constant(config[\"hop_size\"], dtype=tf.int32),\n","        ),\n","        allow_cache=config[\"allow_cache\"],\n","        batch_size=config[\"batch_size\"] * STRATEGY.num_replicas_in_sync,\n","    )\n","\n","    # define trainer\n","    trainer = MelganTrainer(\n","        steps=0,\n","        epochs=0,\n","        config=config,\n","        strategy=STRATEGY,\n","        is_generator_mixed_precision=args.generator_mixed_precision,\n","        is_discriminator_mixed_precision=args.discriminator_mixed_precision,\n","    )\n","\n","    # define generator and discriminator\n","    with STRATEGY.scope():\n","        generator = TFMelGANGenerator(\n","            MELGAN_CONFIG.MelGANGeneratorConfig(**config[\"melgan_generator_params\"]),\n","            name=\"melgan_generator\",\n","        )\n","\n","        discriminator = TFMelGANMultiScaleDiscriminator(\n","            MELGAN_CONFIG.MelGANDiscriminatorConfig(\n","                **config[\"melgan_discriminator_params\"]\n","            ),\n","            name=\"melgan_discriminator\",\n","        )\n","\n","        # dummy input to build model.\n","        fake_mels = tf.random.uniform(shape=[1, 100, 80], dtype=tf.float32)\n","        y_hat = generator(fake_mels)\n","        discriminator(y_hat)\n","\n","        if len(args.pretrained) > 1:\n","            generator.load_weights(args.pretrained)\n","            logging.info(\n","                f\"Successfully loaded pretrained weight from {args.pretrained}.\"\n","            )\n","\n","        generator.summary()\n","        discriminator.summary()\n","\n","        gen_optimizer = tf.keras.optimizers.Adam(**config[\"generator_optimizer_params\"])\n","        dis_optimizer = tf.keras.optimizers.Adam(\n","            **config[\"discriminator_optimizer_params\"]\n","        )\n","\n","    trainer.compile(\n","        gen_model=generator,\n","        dis_model=discriminator,\n","        gen_optimizer=gen_optimizer,\n","        dis_optimizer=dis_optimizer,\n","    )\n","\n","    # start training\n","    try:\n","        trainer.fit(\n","            train_dataset,\n","            valid_dataset,\n","            saved_path=os.path.join(config[\"outdir\"], \"checkpoints/\"),\n","            resume=args.resume,\n","        )\n","    except KeyboardInterrupt:\n","        trainer.save_checkpoint()\n","        logging.info(f\"Successfully saved checkpoint @ {trainer.steps}steps.\")\n"]}]}